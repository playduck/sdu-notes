\documentclass[
    fontsize      = 11pt,
    paper         = a4,
    twoside       = false,
    parskip       = half,
    pagesize      = false,
]{scrartcl}

\author{Robin Prillwitz}

\usepackage[ngerman]{babel}
\usepackage[iso,german]{isodate}
\date{12 August 2022}

\usepackage{hyphenat}
\hyphenation{Mathe-matik wieder-gewinnen}
\usepackage[babel=true]{csquotes}
\usepackage[protrusion=true,expansion,tracking=true,nopatch=eqnum]{microtype}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[locale=DE]{siunitx}

\usepackage[outputdir=./temp/sdu]{minted}

\usepackage{graphicx}
\usepackage{grffile}
\graphicspath{{./img/}}

\usepackage{longtable}
\usepackage{booktabs}

\usepackage[normalem]{ulem}

% % Scale images if necessary, so that they will not overflow the page
% % margins by default, and it is still possible to overwrite the defaults
% % using explicit options in \includegraphics[width, height, ...]{}
% \setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{shapes}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,automata,backgrounds,petri}
\usepackage[european, betterproportions]{circuitikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{pgfgantt}
\usepackage{pgfornament}
\pgfplotsset{
  compat=1.18, % lastest release as of 2022-07-02
}

\usepackage{scrlayer}
\usepackage[]{scrlayer-scrpage}
\ohead{12 August 2022}
\chead{\lowercase{\scshape{sdu}}}
\ihead{Robin Prillwitz}
\ofoot*{\pagemark}
\cfoot*{}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fontspec}
\usepackage{textcomp}

\setsansfont[
    Scale       = MatchLowercase,
    ScaleAgain  = 1.08,
    Ligatures   = TeX
]{Helvetica Neue}

\setmonofont[
    Scale       = MatchLowercase,
    Ligatures   = TeX,
    Contextuals = {Alternate}
]{Fira Code}

\setmainfont[
    Scale       = MatchLowercase,
    UprightFont =  *-Regular,
    BoldFont    =  *-Bold,
    ItalicFont  =  *-It,
    Ligatures   = TeX
]{Minion Pro}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\usepackage[hidelinks]{hyperref}

\begin{document}

\tableofcontents

\clearpage
\newpage

\hypertarget{intro}{%
\section{Intro}\label{intro}}

\hypertarget{requirements}{%
\subsection{Requirements}\label{requirements}}

\begin{itemize}
\tightlist
\item
  Matlab (with Communications Toolbox)
\item
  Putty / SSH Client
\item
  09:00 to (no later than) 17:00
\end{itemize}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

\hypertarget{embodied-ai}{%
\subsubsection{Embodied AI}\label{embodied-ai}}

\hypertarget{biological-neurons}{%
\section{Biological-neurons}\label{biological-neurons}}

\hypertarget{neurons}{%
\subsection{Neurons}\label{neurons}}

\begin{itemize}
\tightlist
\item
  \textbf{Dendrite(s)}: Input(s)
\item
  \textbf{Axion}: Output
\item
  \textbf{Soma}: Cell body
\item
  \textbf{Nucleus}: Cell core
\end{itemize}

\emph{Neurons} collect electrical signals to process and transmit to
other \emph{neurons}. \emph{Axon} terminals of one connect to
\emph{dendrites} of other \emph{neurons}. \emph{Synapses} are structures
to connect those electrically/chemically (however no physical connetion
is made).

\hypertarget{synapse}{%
\subsection{Synapse}\label{synapse}}

Electrical signal trasnission through Ion-filled Substrate.

\begin{itemize}
\tightlist
\item
  \textbf{Pre}\emph{synamptic neuron}: Sending Signals from Axiom
\item
  \textbf{Post}\emph{synamptic neuron}: Receiving Signals at the
  Dendrite
\end{itemize}

Voltage changes open Voltage gates from the neural-fluid into the
\emph{presynaptic neuron}. This pulls in Ions from the neuralfluid maing
\emph{vesicles} release realese \emph{neurotransmitteres} into the
\emph{synaptic cleft} to move between the \emph{neurons}. The
\emph{postsynaptic neuron} receives these trasmitteres into receivers
and converts the chemical information to an electrical signal.

\hypertarget{signals}{%
\subsection{Signals}\label{signals}}

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{neuron-spike.png}
\caption{Neuron Spike}
\end{figure}

\begin{itemize}
\tightlist
\item
  \emph{Resting} at \(-70\si{mV}\)
\item
  \emph{Depolarization phase}: Excitation from input signal reaching an
  artificial threshold resulting in a voltage jump (up to
  \(+40\si{mV}\))
\item
  \emph{Repolarization phase}: Return to resting potential
\item
  \emph{Undershoot (Hyperpolarization)} return to resting.
\end{itemize}

All voltages with respect to outside brainfluid.

This process thakes around \(3\si{ms}\ \left( 333.33\si{Hz} \right)\).
The \emph{Myelin Sheaths} decreases performance and throughput aswell.
This is faster due to thight packing and parallel processing.

The spike is seemingly identical between different neurons (same
Amplitude and timeframe).

\hypertarget{resting-membrane-potential-default-state}{%
\subsubsection{Resting membrane potential (default
state)}\label{resting-membrane-potential-default-state}}

Different ion concentration: more negative inside the neuron than
outside. Measurement in reference to outside. Outside: Mostly
\(\mathit{Na}+\) and \(\textit{Cl}-\). Inside: \(\mathit{K}+\) and
\(\mathit{A}-\). Resting voltage sits at around
\(-65\si{mV} \text{ to } -70\si{mV}\).

\hypertarget{de-polarization}{%
\subsubsection{De-polarization}\label{de-polarization}}

Ions flow through the neuron. Signal excites gates. Gates are
ion-specific and only allow certain kinds of ions. These are
\emph{voltage-gates channels}.

Ions like Sodium (\(\mathit{Na}+\)) enter the neuron resulting in a
positive voltage swing up to \(+40\si{mV}\). Once all Sodium gates are
open the threshold is reached. The gates open with very little voltage.

\hypertarget{hyper-polarization}{%
\subsubsection{Hyper-polarization}\label{hyper-polarization}}

Once the voltage between neuron and outside fluid is positive, the
Sodium gates close (as they're voltage controlled). Respectively the
Potassium (\(\mathit{K}+\)) gates open. Positive charge leaves the
neuron making the voltage drop to below the threshold. At resting
potential the Potassium gates close. Due to the delay in closure
undershoot occurs.

\hypertarget{encoding-information}{%
\subsubsection{Encoding Information}\label{encoding-information}}

Information is seemingly encoded in timeing between pulses. Amplitudes
and Durations of spikes are too simmilar between spikes.

\hypertarget{artifical-neruons}{%
\subsection{Artifical Neruons}\label{artifical-neruons}}

\hypertarget{perceptrons}{%
\subsubsection{Perceptrons}\label{perceptrons}}

A simple neural model.

All dendrites \(u_i\) get weighted \(w_i\) and summed resulting in the
activation \(z\). The summation simulates the \emph{soma} core.
\[z = \sum_{i=1}^{n} \omega_i \cdot u_i\]

The threshold and axiom are simulated by an activation function \(\phi\)
resulting in the \emph{perceptrons'} output \(v\). \[v = \phi(z)\]

Activation functions tend to clamp the output in the range of \(-1\) to
\(1\).

An activation function dictates the output space. A heaviside function
can only output a binary result. Functions with infinite range may
diverge. Sigmoid functions can't overflow however the may saturate. The
computataional cost is quite prohibitive.

\hypertarget{hodgkin-huxley-model}{%
\subsubsection{Hodgkin-Huxley Model}\label{hodgkin-huxley-model}}

\begin{figure} [h]
\centering
    \begin{circuitikz}[]
    \draw (0,0) to [short, o-*] ++(0,1);
    \draw (0,7) to [short, *-o] ++(0,1);
    \draw (-3,7) -- (3,7);
    \draw (-3,1) -- (3,1);
    \draw (-3, 1) to[C] ++(0,6);
    \draw (-1, 1) to[C, *-] ++(0,3) to[R, -*] ++(0,3);
    \draw (1, 1) to[C, *-] ++(0,3) to[R, -*] ++(0,3);
    \draw (3, 1) to[C,] ++(0,3) to[R] ++(0,3);
    \end{circuitikz}
\caption{}
\end{figure}

\hypertarget{artificial-neural-brains}{%
\section{Artificial neural brains}\label{artificial-neural-brains}}

\hypertarget{braitenberg-vehicles}{%
\subsection{Braitenberg Vehicles}\label{braitenberg-vehicles}}

\begin{itemize}
\tightlist
\item
  \textbf{Ipsilateral}: Connections on same side
\item
  \textbf{Contralateral}: Connections cross sides
\item
  \textbf{Excitatory}: Input Increases \textrightarrow~Output Increases
\item
  \textbf{Inhibitory}: Input Increases \textrightarrow~Output Decreases
\end{itemize}

Vehicle emulates simple \(P\)-type control.

Mathematical model includes:

\begin{itemize}
\tightlist
\item
  \(s_x\): Sensor value
\item
  \(v_x\): Output value
\item
  \(k\): Linear proportional gain
\end{itemize}

Mathematical example implementations:

\begin{itemize}
\tightlist
\item
  \textbf{Ipsilateral}: \(v_{\text{left}} \propto s_{\text{left}}\)
\item
  \textbf{Contralateral}: \(v_{\text{left}} \propto s_{\text{right}}\)
\item
  \textbf{Excitatory}: \(v \propto s\)
\item
  \textbf{Inhibitory}: \(v \propto \frac{1}{s}\)
\end{itemize}

\textbf{Pathplanning}: Finding path from known start to known end
including known obstacles.

Complex behavior emerges by combining multiple weighted control loops
running in parallel.

\hypertarget{artificial-learning}{%
\section{Artificial learning}\label{artificial-learning}}

\hypertarget{plasticity}{%
\subsection{Plasticity}\label{plasticity}}

\textbf{Neuroplasticity}: Ability for the brain to re-organize itself in
both \emph{structure} and \emph{function} over time due to external and
internal events. \textbf{Neuroplasticity} is~mechanism behind
``\emph{learning}'' and is happening continuously.

\begin{longtable}[]{@{}ll@{}}
\toprule
\textbf{Structural Plasticity} & ~\textbf{Functional
Plasticity} \\ \addlinespace
\midrule
\endhead
new neural connections & changing existing connections \\ \addlinespace
long-term changes & short term changes \\ \addlinespace
\bottomrule
\end{longtable}

\textbf{Plasticity} happens on all levels from cortical down to the
synaptic level.

\begin{itemize}
\tightlist
\item
  \textbf{cortical}: changing stimulus from limbs triggers different
  existing neurons
\item
  \textbf{synaptic}: changing amount of gates on post-synaptic neurons'
  dendrites
\end{itemize}

\hypertarget{synaptic-strength-in-functional-plasticity}{%
\subsection{\texorpdfstring{\textbf{synaptic strength} in functional
plasticity}{synaptic strength in functional plasticity}}\label{synaptic-strength-in-functional-plasticity}}

\hypertarget{long-term-potentiation-ltp}{%
\subsubsection{\texorpdfstring{Long Term Potentiation
(\emph{LTP})}{Long Term Potentiation (LTP)}}\label{long-term-potentiation-ltp}}

\textbf{HFS}: \(100\) Pulses (over \(1\si{s} \rightarrow 100\si{Hz}\))
as an input to a neuron. The neuron is resting at \(t=0\). The
\textbf{HFS} hits the neuron resulting in an instantaneous output, the
\textbf{LTP}. The neurons output jumps, then receedes and continues to
saturate (Only as long as the \textbf{HFS} is continous.) The
\textbf{synaptic strength} is the chance the output is increased.

A lot of fast input \(\rightarrow\) Big changes and high learning

\textbf{LTP \emph{increases} synaptic strength}

\hypertarget{long-term-depressino-ltd}{%
\subsubsection{\texorpdfstring{Long Term Depressino
(\emph{LTD})}{Long Term Depressino (LTD)}}\label{long-term-depressino-ltd}}

The Inverse, to decrease the \textbf{synaptic strength} an \textbf{LFS}
(\(900\) Pulses \(15\si{min} \rightarrow 1\si{Hz}\)) is sent. The neuron
responsds, dips and saturates in a depression.

Low data \(\rightarrow\) Low learning

\textbf{LTD \emph{decreases} synaptic stregth}

\hypertarget{chemical-basis}{%
\subsubsection{Chemical basis}\label{chemical-basis}}

\textbf{LTP} and \textbf{LTD} result in synapeses by creating or
destroying gates at the pos-synaptic terminal respectively.

\hypertarget{hebbian-learning-model}{%
\subsection{Hebbian learning model}\label{hebbian-learning-model}}

Efficiency describes the likelyhood if a presynaptic neuron spiking and
exciting it's postsynaptic neuron. The likelyhood of the post-synaptic
neuron firing after having been exicted is increased. More firing
together \(\rightarrow\) more likely to fire together in the future.
They spiking is, however, \emph{not necessarily causal}. At high
efficiency the spiking of both neurons are \textbf{temporally
correlated}. The spiking is \textbf{associative} and
\textbf{unsupervised}.

\textbf{Neurons that fire together, wire together.}

\hypertarget{simple-mathematical-model}{%
\subsubsection{Simple mathematical
model}\label{simple-mathematical-model}}

\[\frac{\mathit{d}\omega_1}{\mathit{d}t} = \mu \cdot v \cdot u_1\]

\begin{itemize}
\tightlist
\item
  \(\omega\): dsecribes the synaptic strength / weight
\item
  \(\frac{\mathit{d}\omega_1}{\mathit{d}t}\): (not a derivative), Change
  in synaptic weight
\item
  \(\mu\): Learnig rate (\(\mu \ll 1\) to avoid ``exploding learning
  problem'')
\item
  \(v\): Output of post-synaptic neuron
\item
  \(u_1\): Output of pre-synaptic neuron / input to post-synaptic neuron
\end{itemize}

\[\omega_n = \omega_{n-1} + \frac{\mathit{d}\omega_{n-1}}{\mathit{d}t} = \omega_{n-1} + \mu \cdot v \cdot u_{n-1} \]

\textbf{Problem}: \(\omega_1\) is always increasing, unstable \sout{but
biologically correct}. This is an open control loop.

As this is unsupervised we don't have an error term and can't simply
stop when the model is ``good enough''.

\hypertarget{ltp}{%
\subsubsection{LTP}\label{ltp}}

The further the amount of time between two spikes firing the more the
weight changes. A high \(\delta t\) results in little change, a small
\(\delta t\) results in large changes. At \(\delta t = 0\) maximal
change occurs. The simple model only results in positive change, thus
unstable.

\hypertarget{input-correlation-learning-ico}{%
\subsection{\texorpdfstring{Input correlation learning
(\emph{ICO})}{Input correlation learning (ICO)}}\label{input-correlation-learning-ico}}

Learning rule
\[\frac{\delta w_a}{\delta t} = \eta \cdot f \left( A, t\right) \otimes \frac{\delta f \left( B, t\right) }{\delta t}\]

\begin{itemize}
\tightlist
\item
  \(\eta\): learning rate
\item
  \(f\left( A, t\right) \otimes \frac{\delta f \left( B, t\right) }{\delta t}\):
  Temporal correlation
\item
  \(otimes\): cross correlation
\item
  \(A\): Predictive signal
\item
  \(B\): Reflex signal
\item
  \(Y\): Neuron Ouput
\item
  \(w_a\) weight between \(A\) and \(Y\)
\item
  \(f\) output function of a neuron (including the sigmoid)
\end{itemize}

If we'd like to stop the learning we can assume \(B\) to be constant. We
cannot guarantee \(B \rightarrow 0\) (to stop learning) but we can take
the derivative to stop learning once stimulus ceases change.

This Algorithm \textbf{will converge} to the correct weight.

Output signal is the weighted sum.
\[Y(t) = w_a \cdot f \left( A, t\right) + f \left( B, t\right)\]

\hypertarget{perceptron-learning}{%
\subsubsection{Perceptron learning}\label{perceptron-learning}}

Learning by updating input weights only. Update done using
\textbf{gradient descent}.

Update weight in proportion to contribution to the output. Contribution
is the change in error \(E\) für a given change in \(w\), where the mean
squared error is defined as \[E = \frac{1}{2} \left( t-v\right)^2\]

\begin{itemize}
\tightlist
\item
  \(t\): target output
\item
  \(v\): actual output
\end{itemize}

Determining error requires a known correct output.

\(\rightarrow\) \textbf{supervised learning}

\hypertarget{supervised-unsupervised-learning}{%
\section{Supervised \& unsupervised
learning}\label{supervised-unsupervised-learning}}

\hypertarget{non-linear-actiavtion-function}{%
\subsection{Non-linear actiavtion
function}\label{non-linear-actiavtion-function}}

\textbf{Bias} is activation function x-Offset \textbf{Slope} of
activation function is rarely used.

An example sigmoid with \textbf{bias} and \textbf{slope}.
\[v =  \frac{1}{1+e^{-S(z-b)}}\]

where \(b\) is \textbf{bias} and \(S\) the \textbf{slope}. \textbf{Bias}
can be used as a weight.

\begin{align*}
z &= w_1u_1 + w_2u_2 + \ldots + w_nu_n \\
& \rightarrow (z-b) = w_1u_1 + w_2u_2 + \ldots + w_nu_n - b\\
& \rightarrow (z-b) = w_1u_1 + w_2u_2 + \ldots + w_nu_n - (b-1)\\[2ex]
\rightarrow & (b-1) \text{ splits to } w_{n+1} \text{ and } u_{n+1}
\end{align*}

This results in an additional weighted bias shifting the activation
function resulting in \[z = \sum_{i=1}^{n+1}\omega_i \cdot u_i\]

Each perceptron can implement one \textbf{deciscion boundary}.
\textbf{Deciscion boundaries} seperate inputs into different classes.
The boundary can be shifted by adapting the weights.

By adding more perceptrons the \textbf{deciscion boundaries} dimension
increases. The boundry of 2 neurons results in one-dimensions. 3 Neurons
create a 2-Dimensional \textbf{deciscion boundary}. More Neurons build
more complex spaces.

\hypertarget{designing-a-network}{%
\subsection{Designing a network}\label{designing-a-network}}

\begin{itemize}
\tightlist
\item
  Defined number of inputs
\item
  Defined number of outputs
\item
  Variable hidden layers
\end{itemize}

Hidden layer depends on linearity of the problem. No general solution to
amount of hidden layers. Strategy of trial and error, start with
\(\approx 100\) layers.

\textbf{Deep Neural Networks}: Depth is defined horizontally.

\hypertarget{convolutional-neural-networks}{%
\subsection{Convolutional neural
networks}\label{convolutional-neural-networks}}

Hereby:

\begin{itemize}
\tightlist
\item
  \(u\): Input
\item
  \(v\) Output
\item
  \(x\) Hidden layer
\item
  \(w\) Weight from \(u\) to \(x\)
\item
  \(y\) Weight from \(x\) to \(v\)
\end{itemize}

One \textbf{Iteration} consists of one forward pass and one backwards
pass. One \textbf{Epoch} consists of \textbf{Iterations} for all Items
in the training set.

\hypertarget{forward-propagation}{%
\subsubsection{Forward propagation}\label{forward-propagation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set input
\item
  Calculate for all hidden layers \[x_j = \sum_i u_i w_{ji}\]
\item
  Calculate for all output layers \[v_k = \sum_j x_j w_{kj}\]
\end{enumerate}

\hypertarget{backpropagation}{%
\subsubsection{Backpropagation}\label{backpropagation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate error gradient for all output neurons
  \[E_k^0 = v_k (1-v_k)(t_k-v_k)\]
\item
  Calculate error gradient for hidden layers
  \[E_j^h = x_j(1-x_j) \sum_k E_K^0 y_{kj}\]
\item
  Update weights for outputs \[y\prime_{kj} = y_{kj} + \mu E_k^0 x_j\]
\item
  Update weights for hidden layers
  \[w\prime_{ji} = w_{ji} + \mu E_j^h u_i\]
\end{enumerate}

\hypertarget{vanishing-gradients}{%
\subsection{Vanishing Gradients}\label{vanishing-gradients}}

With a great amount of layers the impact of early neurons (close to the
input) have less effect on the output error and get changed less
resulting in less learning. A high amount of layers does not guarantee
better network performance.

\textbf{Dropout} randomly disables neurons and stops updating their
weights. This does not guarantee better accuracy only better execution
speed.

This only occurs by learning with backpropagation.

Alternative: \textbf{NEAT} (\emph{Neuroevolution of augmenting
topologies}) using generative algorithms. Possibly (not guaranteed)
better performance to optimize output by chaning the entire networks
structure. Worst execution speed and memory performance.

\hypertarget{trainig-proceedure}{%
\subsection{Trainig proceedure}\label{trainig-proceedure}}

Split trainig dataset into two parts to avoid overfitting. Suggested
split:

\begin{itemize}
\tightlist
\item
  \(70\%\) training data
\item
  \(30\%\) testing data
\end{itemize}

Initilize weights to random values.

\begin{itemize}
\tightlist
\item
  \textbf{Training Datasset}: Adjust weights/learn
\item
  \textbf{Testing Dataset}: Testing final solution
\item
  \textbf{Validation Dataset}: Minimize overfitting
\end{itemize}

Always randomize oder of data for every \textbf{epoch}, as netoworks
easily learn patterns.

More complex splitting algorithms and procesdures:

\begin{itemize}
\tightlist
\item
  \textbf{Monte Carlo corss validation} subsamples data randomly into
  its sets.
\item
  \textbf{K-fold corss validataion} divides data into \(k\) subsets,
  trainig it and removing it after training to repeat with the remaining
  \(k-1\) subsets
\item
  \textbf{Leave-p-out cross validation} \(p\) datasamples, use \(n-p\)
  for training, but test and train \(\frac{n!}{p!\cdot (n-p)!}\) times.
  This presents every datapoint equally often and fairly.
\end{itemize}

\end{document}
