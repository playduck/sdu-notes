\documentclass[
    fontsize      = 11pt,
    paper         = a4,
    twoside       = false,
    parskip       = half,
    pagesize      = false,
]{scrartcl}

\author{Robin Prillwitz}

\usepackage[ngerman]{babel}
\usepackage[iso,german]{isodate}
\date{13. August 2022}

\usepackage{hyphenat}
\hyphenation{Mathe-matik wieder-gewinnen}
\usepackage[babel=true]{csquotes}
\usepackage[protrusion=true,expansion,tracking=true,nopatch=eqnum]{microtype}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[locale=DE]{siunitx}

\usepackage[outputdir=/Users/robin/Documents/sdu-notes/temp/sdu]{minted}

\usepackage{graphicx}
\usepackage{grffile}
\graphicspath{{./img/}}

\usepackage{longtable}
\usepackage{booktabs}

\usepackage[normalem]{ulem}

% % Scale images if necessary, so that they will not overflow the page
% % margins by default, and it is still possible to overwrite the defaults
% % using explicit options in \includegraphics[width, height, ...]{}
% \setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{shapes}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,automata,backgrounds,petri}
\usepackage[european, betterproportions]{circuitikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{pgfgantt}
\usepackage{pgfornament}
\pgfplotsset{
  compat=1.18, % lastest release as of 2022-07-02
}

\usepackage{scrlayer}
\usepackage[]{scrlayer-scrpage}
\ohead{13. August 2022}
\chead{\lowercase{\scshape{Sdu}}}
\ihead{Robin Prillwitz}
\ofoot*{\pagemark}
\cfoot*{}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fontspec}
\usepackage{textcomp}

\setsansfont[
    Scale       = MatchLowercase,
    ScaleAgain  = 1.08,
    Ligatures   = TeX
]{Helvetica Neue}

\setmonofont[
    Scale       = MatchLowercase,
    Ligatures   = TeX,
    Contextuals = {Alternate}
]{Fira Code}

\setmainfont[
    Scale       = MatchLowercase,
    UprightFont =  *-Regular,
    BoldFont    =  *-Bold,
    ItalicFont  =  *-It,
    Ligatures   = TeX
]{Minion Pro}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\usepackage[hidelinks]{hyperref}

\begin{document}

\hypertarget{questions-answers}{%
\section{Questions \& Answers}\label{questions-answers}}

\hypertarget{biologically-inspired-robotics}{%
\subsection{Biologically inspired
robotics}\label{biologically-inspired-robotics}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{What is biologically-inspired robotics?}

  Using biological systems as inspiration for robotic design.
\item
  \emph{What is biorobotics?}

  Using the biologically inspired robotic system to better understand
  the original biological system.
\item
  \emph{What is the difference between biorobotics and
  biologically-inspired robotics?}

  Biorobotics combines existing biological systems with mechanical
  systems. Biologically-inspired robotics takes inspiration of
  biological systems for robotics without combining the two.
  \textbf{(?)}
\item
  \emph{What is the reality check in biorobotics?}

  Complex biological behaviour doesn't arise from complex systems but
  rather simple, yet dedicated, systems.
\item
  \emph{What is neurorobotics?}

  The study and application of science and technology of embodied,
  autonomous, brain-inspired algorithms.
\item
  \emph{What is a neurorobot? Can you give an example of a neurorobot?}

  A breitenberg vehicle.
\item
  \emph{What is embodied AI?}

  A purpose built mechanical system in conjunction with an AI
  controller.
\item
  \emph{Can you explain the three-layer embodied AI architecture?}

  \begin{itemize}
  \tightlist
  \item
    The controller acting as a supervisor: The brain
  \item
    The mechanical input and outputs: Motors and Sensor
  \item
    The environment
  \end{itemize}
\item
  \emph{Can you give an example of embodied AI in humans/animals? Can
  you give an example of embodied AI in robots?}

  Gas detecting robot inspired by cockroaches.
\item
  \emph{Why is the environment important in embodied AI?}

  An enviornment influences an agent. An agent must overcome external
  influences or take advantage of them. Example: Seagulls in hovering in
  gusts of wind without effort.
\item
  \emph{What are model-based and model-free approaches in biorobotics?
  Can you use both together in one robot? Can you give an example?}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Model based}: A mathematical model defines the entire
    robotic system.
  \item
    \textbf{Model free}: The robot acts according to some defined rules
    but ``figures out'' how to achive it's objective by itself.
  \end{itemize}

  Both appoaches can be used together. A model-based approach defining
  the systems baseline behviour with the model-free system adapting to
  external changes.

  Examples: 3-D Printer controlling it's nozzle temperatur with a model
  (PID Controller) and minimizing motor vibrations using a learning
  algorithm.
\end{enumerate}

\hypertarget{neurons}{%
\subsection{Neurons}\label{neurons}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{What is a neuron?}

  A biological braincell responsible for processing information.

  It consists of multiple Inputs (\textbf{Dendrites}), one Output
  (\textbf{Axiom}) and it's \textbf{soma} with **nucleus\emph{.}
\item
  \emph{What are the different types of neurons?}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Unipolar}: Inputs, from outside the brain (found in insects)
  \item
    \textbf{Bipolar}: Inputs, from senses (eyes, ears)
  \item
    \textbf{Multipolar}: Within the brain and as outputs to muscles

    \begin{itemize}
    \tightlist
    \item
      \textbf{Pyramidal}: complex though (Cerebrum)
    \item
      \textbf{Purkinje}: reactive action (Cerebellum)
    \end{itemize}
  \end{itemize}
\item
  \emph{How does a neuron work/transmit a signal through itself?}

  Using electrical spikes, presumeably through the spike signal's
  frequency and timing charachteristics, not by it's shape, amplitude or
  phase. \textbf{(?)}
\item
  \emph{How does a neuron forward a signal to other neuron(s)?}

  By using chemical reactions at synapses.
\item
  \emph{What is a synapse?}

  A non-physical connection between two neurons.
\item
  \emph{How does a synapse transmit a signal?}

  The electric signal in the pre-synaptic neuron releases
  neurotransmitters into the synaptic chasm. These travel through the
  brain fluid to the post-synaptic neurons neuroreceptors. If enough
  receptors get stimulated they create an electric spike.
\item
  \emph{What is an action potential?}

  An electrical signal in a neuron.
\item
  \emph{Describe how an action potential is generated.}

  By the neuroreceptors in the post-synaptic neuron.
\item
  \emph{Describe how the different phases of the action potential are
  generated.}

  \begin{itemize}
  \tightlist
  \item
    The \textbf{resting phase} is the default state.
  \item
    The \textbf{depolarization phase} occurs once the Natrium (\(Na+\))
    gates reach a voltage-threshold. A voltage overshoot occurs as
    reactions are not instantenous.
  \item
    The \textbf{repolarization phase} follows showing a decline in
    voltage as the Sodium gates saturate and repell ions, thus allowing
    Potassium (\(K+\)) to enter.
  \item
    The \textbf{hyperpolarization phase} returns to the resting state as
    the voltage reaches an equilibrium.
  \end{itemize}
\item
  \emph{What is resting
  potential/depolarisation/repolarisation/hyperpolarisation? What
  mechanism(s) causes each phase to be generated?}

  \textbf{See above answer}.
\item
  \emph{Why is the resting potential negative?}

  As ions leak through the neurons membrane the neuron takes on a more
  negative charge in respect to the outside fluid.
\item
  \emph{How is information encoded in spikes?}

  Presumeably in its frequency and timing charachteristics.
\item
  \emph{What is a perceptron?}

  A quantized model of a neurons behavior. Consisting of multiple
  weighted inputs getting summed and passed through an activation
  function.
\item
  \emph{What is an activation?}

  The activation describes the weighted sum of the perceptrons inputs.
  \(z = \sum_{i=1}^n \omega_i \cdot u_i\)
\item
  \emph{What is an activation function? Why do we need it/What will
  happen if I don't use it?}

  A mathematical function limiting the perceptrons activation (weighted
  sum) to a knwon output space.
\item
  \emph{What are the different types of activation functions and their
  advantages/drawbacks? Which one will you choose and why?}

  \begin{align*}
   \text{RelU } &= \begin{cases} 0 \text{ for } x < 0\\ x \text{ for } x \geq 0\end{cases}\\[2ex]
   \text{Heaviside } &= \begin{cases} 0 \text{ for } x < 0\\ 0.5 \text{ for } x = 0\\ 1 \text{ for } x > 0\end{cases}\\[2ex]
   \text{Linear } &= x\\
   \text{Sigmoid} &= \frac{1}{1+e^{-x}}
   \end{align*}

  \begin{itemize}
  \tightlist
  \item
    Heaviside limits the output to a binary state, severly reducing
    granularity.
  \item
    Linear and RelU are computationally efficient but may grow
    unbounded.
  \item
    Sigmoids limit the effective range and don't quantize dramatically.
  \end{itemize}
\item
  \emph{Why do we use a sigmoid activation function (think about it from
  both computer implementation and biology perspectives)?}

  It binds the output to a manageable range which avoids overflow and
  offers great precicion using IEE754. It also models neurons' own
  saturation.
\item
  \emph{What is the basic difference between biological neural
  processing and artificial neural processing?}

  Biological neural processing can operate many billions (or more)
  neurons in parallel. Computing with artifical neurons cannot currently
  achieve this.
\item
  \emph{What is a Hodgekin-Huxley model? What
  similarities/dissimilarities does it have with a biological neuron?
  What drawbacks does it have in terms of implementation?}

  An electronic circuit aiming to reproduce neurons' voltage spikes. It
  models different Ion-charges and resistances using batteries and
  resistors acting upon a cpacitor and receiving an external input.

  The model only offers one input. It breaks down with too-high currents
  failing to simulate accordingly. The model only generates spikes and
  it's output is not generally useful for computing.

  It's implementation is computationally incredibly expensive requring
  mutliple equations to be evaluated for every timestep of the voltage
  signal.
\end{enumerate}

\hypertarget{braitenberg-vehicles}{%
\subsection{Braitenberg vehicles}\label{braitenberg-vehicles}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{What is a Braitenberg vehicle? Can you give an example of one
  and how it behaves?}

  A simple concept of a neurorobot consisting of two powered wheels
  receiving input from two sensors placed at the top left and right of
  the vehicle. The connections between the sensors and wheels dictate
  the vehicles emerging behaviour.

  An agressive vehicle features \textbf{contralateral} and
  \textbf{excitatory} connetions and agressively manouvers towards the
  sensor's gratests stimulus. The \glqq love\grqq vehicle consists of
  \textbf{ipsilateral} and \textbf{inhibitory} connections resulting in
  it seeking the source of gratests stimulus but slowly coming to a halt
  when approaching it.
\item
  \emph{Are there any advantages of using Braitenberg vehicles?}

  Their rather complex behavior emerging from very simplistic rules make
  them computationally trivial.

  They enable emulating behaviors of simple insects.
\end{enumerate}

\hypertarget{learning}{%
\subsection{Learning}\label{learning}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{What is neuroplasticity? What types of neuroplasticity are
  there?}

  Neuroplasticity describes the ability of the brain to re-organize
  itself over time in order to learn. There is \textbf{structural} and
  \textbf{functional} neuroplasticity.
\item
  \emph{What is structural/functional plasticity? What are the
  differences between the two?}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Structural}: Major structural changes over long periods of
    time
  \item
    \textbf{Functional}: Minor adaptions over short periods of time
  \end{itemize}

  There are more biological mechanisms for \textbf{structural} than for
  \textbf{functional} changes.
\item
  \emph{Can you give an example of structural/functional plasticity?}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Structural}:

    \begin{itemize}
    \tightlist
    \item
      Synapses changing in number
    \item
      Synapses receptors changing in density
    \end{itemize}
  \item
    \textbf{Functional}:

    \begin{itemize}
    \tightlist
    \item
      Synapses adjusting their strength (\emph{synaptic plasticity})
    \end{itemize}
  \end{itemize}
\item
  \emph{What is Long Term Potentiation? What is Long Term Depression?
  Can you say something about the chemical process and any changes
  underlying LTP and LDP?}

  \textbf{LTP}: If the synapse is overstimulated for a long period of
  time (many pulses in quick succession, high frequency) the synapse
  will create more neural receptors and thus \glqq strengthen \grqq the
  conection. The \emph{fEPSP} slope increases.

  \textbf{LTD}: If the synapse is, however, understimulated for a long
  time (very few pulses, low frequency), it will reduce the ammount of
  neuroreceptors at the post-synaptic neuron essentially
  \glqq weakening\grqq the conection. The \emph{fEPSP} slope decreases.

  \textbf{(?)}
\item
  \emph{What is Hebbian learning? What are its drawbacks as a model for
  learning?}

  \glqq Neurons that fire together, wire together \grqq

  It describes the likelyhood of a presynaptic neuron spiking and
  exciting it's postsynaptic neuron. The likelyhood of the post-synaptic
  neuron firing after having been exicted is increased. More firing
  together \(\rightarrow\) more likely to fire together in the future.
  They spiking is, however, \textbf{not necessarily causal}. At high
  efficiency the spiking of both neurons are \textbf{temporally
  correlated}. The spiking is \textbf{associative} and
  \textbf{unsupervised}.

  It's model is describes as
  \[\frac{\partial \omega_1}{\partial t} = \mu v u_1\] wherby the
  partial term \(\frac{\partial \omega_1}{\partial t}\) may grow
  unbounded. As this control loop is unsupervised we can't stop the
  model once it's \glqq good enough\grqq.
\item
  \emph{What is Spike-Timing Dependent Plasticity?}

  \textbf{STDP} describes neurons' change in synaptic weight in relation
  to \textbf{LT Potentiation} and \textbf{LT Depression}. This proves a
  biological basis for Hebbian learning, as neurons which have a strong
  temporal correlation have a realtively strong synaptic weight.
\item
  \emph{Do you know other forms of functional plasticity?}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Homosynaptic} plasticity: changes in synapse strength occur
    only at post-synaptic targets that are specifically stimulated by a
    pre-synaptic target.
  \item
    \textbf{Heterosynaptic} plasticity: activity of a third neuron can
    releases chemical neuromodulators that induce changes in synaptic
    strength between two other neurons.
  \item
    \textbf{Non-synaptic} plasticity: intrinsic excitability,
    i.e.~sensitivity to synaptic input, of neurons can be altered and is
    manifested as changes in the firing characteristics of the neuron
    itself.
  \item
    \textbf{Homeostatic} plasticity: capacity of neurons to regulate
    their own excitability relative to network activity, a compensatory
    adjustment that occurs over the timescale of days.
  \end{itemize}
\item
  \emph{What type of plasticity is Hebbian learning (homo-, hetero-,
  non- or homeostatic)?}

  Hebbian learning is \textbf{homosynaptic} plasticity. \textbf{(?)}
\item
  \emph{What is ICO learning? What type of synaptic plasticity is it and
  why? Can you give an example of how ICO learning can be used in
  robots?}

  \textbf{Input correlation learning} (\textbf{ICO}) is
  \textbf{heterosynaptic} plasticity.

  The goal is to detect an event which triggers a reflex signal using a
  predictive signal (without even triggering the reflex). This is
  implemented using the correlation of these two (predictive and reflex
  signal). Implementing this requires a third neuron (one for each input
  and one additional neuron as output), thus \textbf{heterosynapsis} is
  required.

  Example: Detecting obstacles using a camera (predictive input) without
  triggering the bump sensor (reflex input). The robot can learn to
  avoid obstacles by using the camera only.
\item
  \emph{How do perceptrons learn? What is the fundamental difference
  between ICO learning and perceptron learning (think in terms of how
  gradient descent works vs how the ICO learning rule works)?}

  \begin{itemize}
  \tightlist
  \item
    \textbf{ICO} learns by adapting the time between predictive input
    and reflex input (the correlation between those). This process is
    \textbf{unsupervised}.
  \item
    \textbf{perceptrons} learn by adjusting their weight by trying to
    minimize some given error function. This requires an expected output
    and is \textbf{supervised}.
  \end{itemize}
\item
  \emph{What is a Multi-Layer Perceptron (MLP)?}

  An \textbf{MLP} is a neural network. It consists of an input layer, an
  output layer and a defined amount of fully convoluted hidden layers
  inbetween those two.
\item
  \emph{What is the backpropagation algorithm and how does it work? Why
  is it called a gradient descent method?}

  Backpropagation describes the process of changing weights in
  accordance to their respective impact on the output error. An
  errorgradient is calculated for each output neuron and for each hidden
  layer up to the input. Based on this gradient each weight is adjusted
  accordingly.

  The gradient describes an \(n\)-dimensional vector pointing towards
  the steepest decent on the error manifold's surface. Decending this is
  called gradient decent.
\end{enumerate}

\end{document}
