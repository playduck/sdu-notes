\documentclass[
    fontsize      = 11pt,
    paper         = a4,
    twoside       = false,
    parskip       = half,
    pagesize      = false,
]{scrartcl}

\author{Robin Prillwitz}

\usepackage[ngerman]{babel}
\usepackage[iso,german]{isodate}
\date{12. August 2022}

\usepackage{hyphenat}
\hyphenation{Mathe-matik wieder-gewinnen}
\usepackage[babel=true]{csquotes}
\usepackage[protrusion=true,expansion,tracking=true,nopatch=eqnum]{microtype}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[locale=DE]{siunitx}

\usepackage[outputdir=/Users/robin/Documents/sdu-notes/temp/sdu]{minted}

\usepackage{graphicx}
\usepackage{grffile}
\graphicspath{{./img/}}

\usepackage{longtable}
\usepackage{booktabs}

\usepackage[normalem]{ulem}

% % Scale images if necessary, so that they will not overflow the page
% % margins by default, and it is still possible to overwrite the defaults
% % using explicit options in \includegraphics[width, height, ...]{}
% \setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{shapes}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,automata,backgrounds,petri}
\usepackage[european, betterproportions]{circuitikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{pgfgantt}
\usepackage{pgfornament}
\pgfplotsset{
  compat=1.18, % lastest release as of 2022-07-02
}

\usepackage{scrlayer}
\usepackage[]{scrlayer-scrpage}
\ohead{12. August 2022}
\chead{\lowercase{\scshape{Sdu}}}
\ihead{Robin Prillwitz}
\ofoot*{\pagemark}
\cfoot*{}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fontspec}
\usepackage{textcomp}

\setsansfont[
    Scale       = MatchLowercase,
    ScaleAgain  = 1.08,
    Ligatures   = TeX
]{Helvetica Neue}

\setmonofont[
    Scale       = MatchLowercase,
    Ligatures   = TeX,
    Contextuals = {Alternate}
]{Fira Code}

\setmainfont[
    Scale       = MatchLowercase,
    UprightFont =  *-Regular,
    BoldFont    =  *-Bold,
    ItalicFont  =  *-It,
    Ligatures   = TeX
]{Minion Pro}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\usepackage[hidelinks]{hyperref}

\begin{document}

\hypertarget{supervised-unsupervised-learning}{%
\section{Supervised \& unsupervised
learning}\label{supervised-unsupervised-learning}}

\hypertarget{non-linear-actiavtion-function}{%
\subsection{Non-linear actiavtion
function}\label{non-linear-actiavtion-function}}

\textbf{Bias} is activation function x-Offset \textbf{Slope} of
activation function is rarely used.

An example sigmoid with \textbf{bias} and \textbf{slope}.
\[v =  \frac{1}{1+e^{-S(z-b)}}\]

where \(b\) is \textbf{bias} and \(S\) the \textbf{slope}. \textbf{Bias}
can be used as a weight.

\begin{align*}
z &= w_1u_1 + w_2u_2 + \ldots + w_nu_n \\
& \rightarrow (z-b) = w_1u_1 + w_2u_2 + \ldots + w_nu_n - b\\
& \rightarrow (z-b) = w_1u_1 + w_2u_2 + \ldots + w_nu_n - (b-1)\\[2ex]
\rightarrow & (b-1) \text{ splits to } w_{n+1} \text{ and } u_{n+1}
\end{align*}

This results in an additional weighted bias shifting the activation
function resulting in \[z = \sum_{i=1}^{n+1}\omega_i \cdot u_i\]

Each perceptron can implement one \textbf{deciscion boundary}.
\textbf{Deciscion boundaries} seperate inputs into different classes.
The boundary can be shifted by adapting the weights.

By adding more perceptrons the \textbf{deciscion boundaries} dimension
increases. The boundry of 2 neurons results in one-dimensions. 3 Neurons
create a 2-Dimensional \textbf{deciscion boundary}. More Neurons build
more complex spaces.

\hypertarget{designing-a-network}{%
\subsection{Designing a network}\label{designing-a-network}}

\begin{itemize}
\tightlist
\item
  Defined number of inputs
\item
  Defined number of outputs
\item
  Variable hidden layers
\end{itemize}

Hidden layer depends on linearity of the problem. No general solution to
amount of hidden layers. Strategy of trial and error, start with
\(\approx 100\) layers.

\textbf{Deep Neural Networks}: Depth is defined horizontally.

\hypertarget{convolutional-neural-networks}{%
\subsection{Convolutional neural
networks}\label{convolutional-neural-networks}}

Hereby:

\begin{itemize}
\tightlist
\item
  \(u\): Input
\item
  \(v\) Output
\item
  \(x\) Hidden layer
\item
  \(w\) Weight from \(u\) to \(x\)
\item
  \(y\) Weight from \(x\) to \(v\)
\end{itemize}

One \textbf{Iteration} consists of one forward pass and one backwards
pass. One \textbf{Epoch} consists of \textbf{Iterations} for all Items
in the training set.

\hypertarget{forward-propagation}{%
\subsubsection{Forward propagation}\label{forward-propagation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set input
\item
  Calculate for all hidden layers \[x_j = \sum_i u_i w_{ji}\]
\item
  Calculate for all output layers \[v_k = \sum_j x_j w_{kj}\]
\end{enumerate}

\hypertarget{backpropagation}{%
\subsubsection{Backpropagation}\label{backpropagation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate error gradient for all output neurons
  \[E_k^0 = v_k (1-v_k)(t_k-v_k)\]
\item
  Calculate error gradient for hidden layers
  \[E_j^h = x_j(1-x_j) \sum_k E_K^0 y_{kj}\]
\item
  Update weights for outputs \[y\prime_{kj} = y_{kj} + \mu E_k^0 x_j\]
\item
  Update weights for hidden layers
  \[w\prime_{ji} = w_{ji} + \mu E_j^h u_i\]
\end{enumerate}

\hypertarget{vanishing-gradients}{%
\subsection{Vanishing Gradients}\label{vanishing-gradients}}

With a great amount of layers the impact of early neurons (close to the
input) have less effect on the output error and get changed less
resulting in less learning. A high amount of layers does not guarantee
better network performance.

\textbf{Dropout} randomly disables neurons and stops updating their
weights. This does not guarantee better accuracy only better execution
speed.

This only occurs by learning with backpropagation.

Alternative: \textbf{NEAT} (\emph{Neuroevolution of augmenting
topologies}) using generative algorithms. Possibly (not guaranteed)
better performance to optimize output by chaning the entire networks
structure. Worst execution speed and memory performance.

\hypertarget{trainig-proceedure}{%
\subsection{Trainig proceedure}\label{trainig-proceedure}}

Split trainig dataset into two parts to avoid overfitting. Suggested
split:

\begin{itemize}
\tightlist
\item
  \(70\%\) training data
\item
  \(30\%\) testing data
\end{itemize}

Initilize weights to random values.

\begin{itemize}
\tightlist
\item
  \textbf{Training Datasset}: Adjust weights/learn
\item
  \textbf{Testing Dataset}: Testing final solution
\item
  \textbf{Validation Dataset}: Minimize overfitting
\end{itemize}

Always randomize oder of data for every \textbf{epoch}, as netoworks
easily learn patterns.

More complex splitting algorithms and procesdures:

\begin{itemize}
\tightlist
\item
  \textbf{Monte Carlo corss validation} subsamples data randomly into
  its sets.
\item
  \textbf{K-fold corss validataion} divides data into \(k\) subsets,
  trainig it and removing it after training to repeat with the remaining
  \(k-1\) subsets
\item
  \textbf{Leave-p-out cross validation} \(p\) datasamples, use \(n-p\)
  for training, but test and train \(\frac{n!}{p!\cdot (n-p)!}\) times.
  This presents every datapoint equally often and fairly.
\end{itemize}

\end{document}
